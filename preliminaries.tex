\section{Preliminaries}
\label{sec:preliminaries}

\begin{definition}[Markov chains]
\label{def:Markov Chains}
A Markov Chain $\mathcal{C}$ is a tuple $(Q, \delta)$ where Q is a finite set of states and 
$\delta: Q \rightarrow \mathbb{D}(Q)$ is a probabilistic transition function, where 
$\mathbb{D}(Q)$ is the set of all probabilistic distribution on a finite set $Q$. A probabilistic distribution on 
$Q$, is a function $f: Q\rightarrow\mathbb{Q}_{\geq0}$ such that $\sum_{q\in Q}f(q)=1$. 
\end{definition}

\noindent
A non-empty finite word $\varrho = p_1p_2...p_n$ over $Q$ is defined as a run of Markov Chain. 
The probability of the run is $\prod_{0\leq i < n}\delta(p_i,p_{i+1})$. 
$\varrho$ reaches $q$, if $q = p_i$ for some $0\leq i\leq n$. 
\newline
\\
The probability of eventually reaching a set of state $T\subseteq Q$ 
in $\mathcal{C}$ starting from $q_0$ can be denoted as $\mathbb{P}^{q_0}_\mathcal{C}[\lozenge T]$. $\lozenge T$ is 
equivalent to $Q\bigcup T$, Q until T. Since not necessary every state in Q is reached before T, 
a set of states $S\subseteq Q$ is defined, where the path until T only consists of these states.
The probability $\mathbb{P}^{q_0}_\mathcal{C}[S\bigcup T]$. If $q_0 \in T$, 
the probability is 1. Otherwise, the probability is calculated as follows: 
\begin{align*}
    \sum\biggl\{ \prod_{0\leq i< n}\delta(q_i,q_{i+1}) ~|~ q_0...q_{n-1}\in (S~\backslash~ T) ~\&~ q_n 
    \in T\ \text{ for } n\geq 1\biggr\}.
\end{align*}

\noindent
If there exists a set $U\subseteq Q$, where all runs from $q_0$ to $T$ reaches, the probability of 
$\mathbb{P}^{q_0}_\mathcal{C}[\lozenge T]$ can be reduced to the following lemma:  

\begin{lemma}
\label{lemma 1}
Consider a Markov Chain $\mathcal{C}=(Q,\delta)$ set of states $U,T\subseteq Q$, and a state $q_0\in Q\backslash U$. 
If $\mathbb{P}^{q_0}_\mathcal{C}[(Q\backslash U)\bigcup T]=0$, then 
\begin{align*}
    \mathbb{P}^{q_0}_\mathcal{C}[\lozenge T] = \sum_{u\in U}\mathbb{P}^{q_0}_\mathcal{C}[(Q\backslash U) \bigcup u]
    \mathbb{P}^{q_0}_\mathcal{C}[\lozenge T]
\end{align*}
\end{lemma}


\begin{definition}[Markov decision processes]
\label{Markov decision processes}
A (finite and discrete-time) Markov decision processes, MDP, M, is a tuple $(Q,A,\delta,T)$ 
where Q is a finite set of states, A a finite set of actions, $\delta: Q \times A \rightarrow \mathbb{D}(Q)$ 
a probabilistic transition function, and $T\subseteq Q$ a set of target state.
\end{definition}

\noindent
The notation for the probability of the state p reaching q with the action a, $\delta(p,a)(q)$ will be changed to 
$\delta(q|p,a)$ for convenience.

\begin{definition}[Strategies]
\label{Strategies}
A (memoryless deterministic) strategy $\sigma$ in an MDP M = $(Q,A,\delta,T)$ is a function $\sigma: Q\rightarrow A$.
\end{definition}

\textit{From MDPs to Chains}


